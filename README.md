# トークナイザーの構築
## 構築方針
### 語彙数
- 語彙数は50,000〜60,000程度にしました。
- このサイズとした背景は以下です。
- 今回は10Bモデル想定と7Bよりやや大きいモデルになる。
- google gemma（vocab: 約25.6万）やQwen（vocab: 約15万）といった最近の高性能なモデルが語彙数を大きめにしている傾向があると感じる。
- 言語的な性質として漢字・カタカナ・ひらがなを持つ日本語という言語は、英語よりも一般的に必要とされる語彙数が多いのではないかと考え、（今回は日本語スクラッチ開発モデルであるため）日本語継続事前学習モデルよりも多めに日本語語彙が入るようにしようと考えた。
- 参考：他モデルの語彙数
    - Swallow：48,000（Llama2の32,000+15,000）
    - Elyza-Llama2-fast：45,043（Llama2の32,000+13,043）
    - LLM-jp：50,570（日本語48.5%, 英語48%, コード3.5%）
### 言語の割合
- チームでは事前学習に使うデータの言語割合を日本語と英語で1:1にするという方針であったため、語彙の割合もそれに近づけるようにしました。
- 日本語のテキストには英語（アルファベット）が入ることが多いため、結果的には（概算ですが）4:6くらいの割合になりました。
- 数式やプログラムに関するトークンも語彙に含めたかったため、1~2%程度が記号・数字などになっています。
### その他
- 使用するツールは標準コードおよび各所でよく使われているSentencePieceとしました。
- アルゴリズムはBPEとUnigramを検討しまたが、以下の背景からUnigramを選択しました。
    - llm-jp-tokenizerライブラリがunigramを前提としていた。
    - "Tokenizer Choice For LLM Training: Negligible or Crucial?"という論文で、英語単体とヨーロッパ圏を含めた多言語での下流タスク性能を比較しており、英語単体ではBPEが優れていたが、多言語ではunigramの方がよかったと述べられていた。
- 他モデルを参考に、正規化はなし（全角と半角を別に扱う）ように設定した。

## 構築手順
## 1. データ準備
比較的高品質と思われるテキストデータを学習に使いました。また、数式やプログラムに頻出するトークンを語彙に含めたいという理由から、一部算数データやプログラムデータを用いました。
- 英語：wiki-40b-en（1.33GB）
- 日本語：wiki-40b-ja（1.78GB）
- プログラム：mbpp（172KB）
- 算数：grade-school-math（2.1MB）

## 2. データ前処理（日本語のみ）
- 日本語は形態素の大きさで分割するために、fugashiも用いて事前に処理しました。
- "||||"という文字列で形態素間の分割を行いました。

## 3. トークナイザーの学習
- 言語間での語彙の割合を任意にしたかったため、言語ごとでトークナイザーを学習して後でマージするという方法を取りました。（参考：llm-jp-tokenizer）


